---
layout: page
title: Publications
permalink: /publications/
weight: 2
---
References
==========

@inproceedings{amit2010iommu,
  title={IOMMU: Strategies for mitigating the IOTLB bottleneck},
  author={Amit, Nadav and Ben-Yehuda, Muli and Yassour, Ben-Ami},
  booktitle={Computer Architecture},
  pages={256--274},
  year=2010,
  organization={Springer Berlin Heidelberg},
  abstract={The input/output memory management unit (IOMMU) was recently introduced into mainstream computer architecture when both Intel and AMD added IOMMUs to their chip-sets. An IOMMU provides memory protection from I/O devices by enabling system software to control which areas of physical memory an I/O device may access. However, this protection incurs additional direct memory access (DMA) overhead due to the required address resolution and validation. IOMMUs include an input/output translation lookaside buffer (IOTLB) to speed-up address resolution, but still every IOTLB cache-miss causes a substantial increase in DMA latency and performance degradation of DMA-intensive workloads. In this paper we first demonstrate the potential negative impact of IOTLB cachemisses on workload performance. We then propose both system software and hardware enhancements to reduce IOTLB miss rate and accelerate address resolution. These enhancements can lead to a reduction of over 60% in IOTLB miss-rate for common I/O intensive workload.}
}

@inproceedings{amit2011viommu,
  title={vIOMMU: efficient IOMMU emulation},
  author={Amit, Nadav and Ben-Yehuda, Muli and Tsafrir, Dan and Schuster, Assaf},
  booktitle={USENIX Annual Technical Conference (ATC)},
  pages={73--86},
  year=2011,
  abstract={Direct device assignment, where a guest virtual machine directly interacts with an I/O device without host intervention, is appealing, because it allows an unmodified (non-hypervisor-aware) guest to achieve near-native performance. But device assignment for unmodified guests suffers from two serious deficiencies: (1) it requires pinning all of the guest's pages, thereby disallowing memory overcommitment, and (2) it exposes the guest's memory to buggy device drivers. We solve these problems by designing, implementing, and exposing an emulated IOMMU (vIOMMU) to the unmodified guest. We employ two novel optimizations to make vIOMMU perform well: (1) waiting a few milliseconds before tearing down an IOMMU mapping in the hope it will be immediately reused ("optimistic tear-down"), and (2) running the vIOMMU on a sidecore, and thereby enabling for the first time the use of a sidecore by unmodified guests. Both optimizations are highly effective in isolation. The former allows bare-metal to achieve 100% of a 10Gbps line rate. The combination of the two allows an unmodified guest to do the same.}
}

@inproceedings{gordon2012eli,
  title={ELI: Bare-Metal Performance for I/O Virtualization},
  author={Gordon, Abel and Amit, Nadav and Har’El, Nadav and Ben-Yehuda, Muli and Landau, Alex and Schuster, Assaf and Tsafrir, Dan},
  booktitle={Architectural Support for Programming Languages \& Operating Systems (ASPLOS)},
  year=2012,
  organization={ACM},
  abstract={Direct device assignment enhances the performance of guest virtual machines by allowing them to communicate with I/O devices without host involvement. But even with device assignment, guests are still unable to approach bare-metal performance, because the host intercepts all interrupts, including those interrupts generated by assigned devices to signal to guests the completion of their I/O requests. The host involvement induces multiple unwarranted guest/host context switches, which significantly hamper the performance of I/O intensive workloads. To solve this problem, we present ELI (ExitLess Interrupts), a software-only approach for handling interrupts within guest virtual machines directly and securely. By removing the host from the interrupt handling path, ELI manages to improve the throughput and latency of unmodified, untrusted guests by 1.3x-1.6x, allowing them to reach 97%-100% of bare-metal performance even for the most demanding I/O-intensive workloads.}
}

@inproceedings{rosenfeld2013using,
  title={Using Disk Add-Ons to Withstand Simultaneous Disk Failures with Fewer Replicas},
  author={Rosenfeld, Eitan and Amit, Nadav and Tsafrir, Dan},
  booktitle={Workshop on the Interaction amongst Virtualization, Operating Systems and Computer Architecture (WIVOSCA)},
  year=2013,
  abstract={Contemporary storage systems that utilize replication often maintain more than two replicas of each data item, reducing the risk of permanent data loss due to simultaneous disk failures. The price of the additional copies is smaller usable storage space, increased network traffic, and higher power consumption. We propose to alleviate this problem with SIMFAIL, a storage system that maintains only two replicas and utilizes per-disk “add-ons”, which are simple hardware devices equipped with relatively small memory that proxy disk I/O traffic. SIMFAIL can significantly reduce the risk of data loss due to temporally adjacent disk failures by quickly copying at risk data from disks to their add-ons. SIMFAIL can further eliminate the risk entirely by maintaining local parity information of disks on their add-ons (such that each add-on holds the parity of its own disk’s data chunks). We postulate that SIMFAIL may open the door for cloud providers to reduce the number of data replicas they use from three to two.}
}

@patent{amit2013enhancing,
  title={ENHANCING INTERRUPT HANDLING IN A VIRTUAL ENVIRONMENT},
  author={Amit, Nadav and Ben-yehuda, Shmuel and Gordon, Abel and Har'el, Nadav Yosef and Landau, Alexander},
  year=2013,
  number={US Patent 20,130,174,148},
  abstract={Systems and methods for enhancing the handling of interrupts in a virtual computing environment are disclosed. A CPU is configured such that the CPU, when in a virtual machine (VM) mode, directs an interrupt to a VM. When in the VM context, a guest in the VM is run with a hypervisor interrupt descriptor table (hypervisor IDT) to determine how the interrupt should be handled. The hypervisor IDT directs an interrupt that is to be handled by the VM to an interrupt handler in a guest IDT without causing a transition to the hypervisor. If an interrupt is to be handled by the hypervisor, the hypervisor IDT causes a transition to the hypervisor.}
}

@inproceedings{amit2014vswapper,
  title={VSWAPPER: A Memory Swapper for Virtualized Environments},
  author={Amit, Nadav and Tsafrir, Dan and Schuster, Assaf},
  booktitle={Architectural Support for Programming Languages \& Operating Systems (ASPLOS)},
  pages={349--366},
  year=2014,
  organization={ACM},
  abstract={The number of guest virtual machines that can be consolidated on one physical host is typically limited by the memory size, motivating memory overcommitment. Guests are given a choice to either install a "balloon" driver to coordinate the overcommitment activity, or to experience degraded performance due to uncooperative swapping. Ballooning, however, is not a complete solution, as hosts must still fall back on uncooperative swapping in various circumstances. Additionally, ballooning takes time to accommodate change, and so guests might experience degraded performance under changing conditions.

Our goal is to improve the performance of hosts when they fall back on uncooperative swapping and/or operate under changing load conditions. We carefully isolate and characterize the causes for the associated poor performance, which include various types of superfluous swap operations, decayed swap file sequentiality, and ineffective prefetch decisions upon page faults. We address these problems by implementing VSWAPPER, a guest-agnostic memory swapper for virtual environments that allows efficient, uncooperative overcommitment. With inactive ballooning, VSWAPPER yields up to an order of magnitude performance improvement. Combined with ballooning, VSWAPPER can achieve up to double the performance under changing load conditions.}
}

@inproceedings{malka2015efficient,
  title={Efficient Intra-Operating System Protection Against Harmful DMAs},
  author={Malka, Moshe and Amit, Nadav and Tsafrir, Dan},
  booktitle={USENIX File and Storage Technologies (FAST)},
  abstract={Operating systems can defend themselves against misbehaving I/O devices and drivers by employing intra-OS protection. With “strict” intra-OS protection, the OS uses the IOMMU to map each DMA buffer immediately before the DMA occurs and to unmap it immediately after. Strict protection is costly due to IOMMU-related hardware over- heads, motivating “deferred” intra-OS protection, which trades off some safety for performance.

We investigate the Linux intra-OS protection mapping layer and discover that hardware overheads are not exclusively to blame for its high cost. Rather, the cost is amplified by the I/O virtual address (IOVA) allocator, which regularly induces linear complexity. We find that the nature of IOVA allocation requests is inherently simple and constrained due to the manner by which I/O devices are used, allowing us to deliver constant time complexity with a compact, easy-to-implement optimization. Our optimization improves the throughput of standard benchmarks by up to 5.5x. It delivers strict protection with performance comparable to that of the baseline deferred protection. To generalize our case that OSes drive the IOMMU with suboptimal software, we additionally investigate the FreeBSD mapping layer and obtain similar findings.},
  year=2015
}

@inproceedings{malka2015riommu,
  title={rIOMMU: Efficient IOMMU for I/O Devices that Employ Ring Buffers},
  author={Malka, Moshe and Amit, Nadav and Ben-Yehuda, Muli and Tsafrir, Dan},
  booktitle={Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={355--368},
  year=2015,
  organization={ACM},
  abstract={The IOMMU allows the OS to encapsulate I/O devices in their own virtual memory spaces, thus restricting their DMAs to specific memory pages. The OS uses the IOMMU to protect itself against buggy drivers and malicious/errant devices. But the added protection comes at a cost, degrading the throughput of I/O-intensive workloads by up to an order of magnitude. This cost has motivated system designers to trade off some safety for performance, e.g., by leaving stale information in the IOTLB for a while so as to amortize costly invalidations. 

We observe that high-bandwidth devices–-like network and PCIe SSD controllers–-interact with the OS via circular ring buffers that that induce a sequential, predictable workload. We design a ring IOMMU (rIOMMU) that leverages this characteristic by replacing the virtual memory page table hierarchy with a circular, flat table. A flat table is adequately supported by exactly one IOTLB entry, making every new translation an implicit invalidation of the former and thus requiring explicit invalidations only at the end of I/O bursts. Using standard networking benchmarks, we show that rIOMMU provides up to 7.56x higher throughput relative to the baseline IOMMU, and that it is within 0.77–1.00x the throughput of a system without IOMMU protection.}
}

@inproceedings{amit2015virtual,
  title={Virtual CPU validation},
  author={Amit, Nadav and Tsafrir, Dan and Schuster, Assaf and Ayoub, Ahmad and Shlomo, Eran},
  booktitle={Proceedings of the 25th Symposium on Operating Systems Principles},
  pages={311--327},
  year=2015,
  organization={ACM},
  abstract={Testing the hypervisor is important for ensuring the correct operation and security of systems, but it is a hard and challenging task. We observe, however, that the challenge is similar in many respects to that of testing real CPUs. We thus propose to apply the testing environment of CPU vendors to hypervisors. We demonstrate the advantages of our proposal by adapting Intel’s testing facility to the Linux KVM hypervisor. We uncover and fix 117 bugs, six of which are security vulnerabilities. We further find four flaws in Intel virtualization technology, causing a disparity between the observable behavior of code running on virtual and bare-metal servers.}
}

@article{amit2016bare,
  title={Bare-Metal Performance for Virtual Machines with Exitless Interrupts},
  author={Amit, Nadav and Gordon, Abel and Har’El, Nadav and Ben-Yehuda, Muli and Landau, Alex and Schuster, Assaf and Tsafrir, Dan},
  booktitle={Communications of the ACM (CACM)},
  volume={59},
  number={1},
  pages={108--116},
  year=2016,
  publisher={ACM},
  abstract={Direct device assignment enhances the performance of guest virtual machines by allowing them to communicate with I/O devices without host involvement. But even with device assignment, guests are still unable to approach bare-metal performance, because the host intercepts all interrupts, including those generated by assigned devices to signal to guests the completion of their I/O requests. The host involvement induces multiple unwarranted guest/host context switches, which significantly hamper the performance of I/O intensive workloads. To solve this problem, we present ExitLess Interrupts (ELI), a software-only approach for handling interrupts within guest virtual machines directly and securely. By removing the host from the interrupt handling path, ELI improves the throughput and latency of unmodified, untrusted guests by 1.3×–1.6×, allowing them to reach 97–100% of bare-metal performance even for the most demanding I/O-intensive workloads.}
}

@phdthesis{amit2014alleviating,
  title={Alleviating Virtualization Bottlenecks},
  author={Amit, Nadav},
  year={2014},
  publisher={Technion-Israel Institute of Technology, Faculty of Computer Science},
  abstract={Hardware virtualization has long been studied, but has only recently become popular, after being introduced to commodity servers. Despite the ongoing research and the developing hardware support, virtual machines incur degraded performance in a wide variety of cases, especially when an unmodified virtual machine operating system is used. One of the major causes of this degraded performance is the lack of physical hardware transparency in virtual machines, since the hypervisor—their controlling software-layer—usually exposes hardware abstractions instead of the physical hardware. While such abstractions are often required to multiplex hardware in virtualization environments, they introduce inefficiencies.
In our work we investigate a wide variety of scenarios in which the lack of transparency incurs substantial performance overheads: I/O memory management unit (IOMMU) emulation, interrupts multiplexing by the hypervisor and memory over-provisioning. For each of these scenarios we suggest novel methods to increase transparency without the virtual machine’s cooperation, and thereby improve performance without modifying its operating system and without access to its source code. Accordingly, the methods we propose apply to proprietary operating systems as well and ease the porting of virtual machines from one hypervisor to another.
First, we show that virtual machine performance with IOMMU emulation, which enhances security, can improve by up to 200% using a novel sidecore emulation approach— performing device emulation by another core instead of the hypervisor. Second, we present a secure and efficient method for selective delivery of interrupts to virtual machines, improving performance by up to 60% for I/O intensive workloads. Last, we introduce VSWAPPER, an efficient uncooperative swapping extension that enhances VM performance when memory is overcommitted by up to an order of magnitude.}
}

@inproceedings{lesokhin2017page,
  title={Page Fault Support for Network Controllers},
  author={Lesokhin, Ilya and Eran, Haggai and Raindel, Shachar and Shapiro, Guy and Grimberg, Sagi and Liss, Liran and Ben-Yehuda, Muli and Amit, Nadav and Tsafrir, Dan},
  booktitle={ACM Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  pages={449--466},
  year={2017},
  abstract={Direct network I/O allows network controllers (NICs) to expose multiple instances of themselves, to be used by untrusted software without a trusted intermediary. Direct I/O thus frees researchers from legacy software, fueling studies that innovate in multitenant setups. Such studies, however, overwhelmingly ignore one serious problem: direct memory accesses (DMAs) of NICs disallow page faults, forcing systems to either pin entire address spaces to physical memory and thereby hinder memory utilization, or resort to APIs that pin/unpin memory buffers before/after they are DMAed, which complicates the programming model and hampers performance. We solve this problem by designing and implementing page fault support for InfiniBand and Ethernet NICs. A main challenge we tackle—unique to NICs—is handling receive DMAs that trigger page faults, leaving the NIC without memory to store the incoming data. We demonstrate that our solution provides all the benefits associated with “regular” virtual memory, notably (1) a simpler programming model that rids users from the need to pin, and (2) the ability to employ all the canonical memory optimizations, such as memory overcommitment and demand-paging based on actual use. We show that, as a result, benchmark performance improves by up to 1.9x.}
}

@patent{schuster2015memory,
  title={Memory swapper for virtualized environments},
  author={Schuster, Assaf and Amit, Nadav and Tsafrir, Dan},
  year={2015},
  number={US9811268B2},
  abstract={A method for reducing disk read rate by managing dataset mapping of virtual machine (VM) guest memory, comprising: monitoring a plurality of disk read write operations of a VM guest; updating a dataset mapping between disk blocks allocated to the VM guest and corresponding physical addresses of memory pages of the VM guest containing replica of data stored in the disk blocks, based on the plurality of disk read write operations; when identifying writing to one of the memory pages, removing a mapping of corresponding disk block and corresponding physical address of memory page; when reclaiming a mapped memory page of the VM guest by a host of the VM guest, discarding data contained in the memory page; and when the data is requested by the VM guest after it was reclaimed by said host, retrieving the data from corresponding disk block according to the mapping.}
}

@inproceedings{amit2017hypercallbacks,
  title={Hypercallbacks: Decoupling Policy Decisions and Execution},
  author={Amit, Nadav and Wei, Michael and Cheng-Chun, Tu},
  booktitle={ACM Workshop on Hot Topics in Operating Systems (HotOS)},
  year=2017,
  abstract={Hypervisors and virtual machines (VMs) running under them must coordinate policy decisions in order to run efficiently. The abstraction of a VM, however, creates a semantic gap which makes it difficult for hypervisor and VM to work in unison due to privilege separation. Today, the semantic gap is bridged by techniques which couple policy decision with execution. In this paper, we introduce a new mechanism, the hypercallback, which enables hypervisors and VMs to coordinate policy with verified, safety-checked code, decoupling execution and decision making. Our preliminary results show that hypercallbacks can significantly improve memory management without compromising security and robustness, and we believe hypercallbacks can be applied to many domains outside of memory management.}
}

@inproceedings{aguilera2017remote,
  title={Remote memory in the age of fast networks},
  author={Aguilera, Marcos K. and Amit, Nadav and Calciu, Irina and Deguillard, Xavier and Gandhi, Jayneel and Subrahmanyam, Pratap and Suresh, Lalith and Tati, Kiran and Venkatasubramanian, Rajesh and Wei, Michael},
  booktitle={ACM Symposium on Cloud Computing (SOCC)},
  year=2017,
  abstract={As the latency of the network approaches that of memory, it becomes increasingly attractive for applications to use remote memory—random-access memory at another computer that is accessed using the virtual memory subsystem. This is an old idea whose time has come, in the age of fast networks. To work effectively, remote memory must address many technical challenges. In this paper, we enumerate these challenges, discuss their feasibility, explain how some of them are addressed by recent work, and indicate other promising ways to tackle them. Some challenges remain as open problems, while others deserve more study. In this paper, we hope to provide a broad research agenda around this topic, by proposing more problems than solutions.}
}

@inproceedings{amit2017optimizing,
  title={Optimizing the TLB Shootdown Algorithm with Page Access Tracking},
  author={Amit, Nadav},
  booktitle={USENIX Annual Technical Conference (ATC)},
  abstract={The operating system is tasked with maintaining the coherency of per-core TLBs, necessitating costly synchronization operations, notably to invalidate stale mappings. As core-counts increase, the overhead of TLB synchronization likewise increases and hinders scalability, whereas existing software optimizations that attempt to alleviate the problem (like batching) are lacking.

We address this problem by revising the TLB synchronization subsystem. We introduce several techniques that detect cases whereby soon-to-be invalidated mappings are cached by only one TLB or not cached at all, allowing us to entirely avoid the cost of synchronization. In contrast to existing optimizations, our approach leverages hardware page access tracking. We implement our techniques in Linux and find that they reduce the number of TLB invalidations by up to 98% on average and thus improve performance by up to 78%. Evaluations show that while our techniques may introduce overheads of up to 9% when memory mappings are never removed, these overheads can be avoided by simple hardware enhancements.},
  year=2017
}

@article{amit2017hypercallbacks-osr,
  title={Hypercallbacks},
  author={Amit, Nadav and Wei, Michael and Tu, Cheng-Chun},
  journal={ACM SIGOPS Operating Systems Review},
  volume={51},
  number={1},
  pages={54--59},
  year=2017,
  publisher={ACM},
  abstract={Hypervisors and virtual machines (VMs) running under them must coordinate policy decisions in order to run efficiently. The abstraction of a VM, however, creates a semantic gap which makes it difficult for hypervisor and VM to work in unison due to privilege separation. Today, the semantic gap is bridged by techniques which couple policy decision with execution. In this paper, we introduce a new mechanism, the hypercallback, which enables hypervisors and VMs to coordinate policy with verified, safety-checked code, decoupling execution and decision making. Our preliminary results show that hypercallbacks can significantly improve memory management without compromising security and robustness, and we believe hypercallbacks can be applied to many domains outside of memory management.}
}

@inproceedings{amit2017hypercallbacks-systex,
  title={Hypercallbacks: A New Mechanism for Trusted, Secure Introspection},
  author={Amit, Nadav and Wei, Michael},
  booktitle={Workshop on System Software for Trusted Execution (SysTEX)},
  year=2017
}

@inproceedings{amit2018design,
  title={The Design and Implementation of Hyperupcalls},
  author={Amit, Nadav and Wei, Michael},
  booktitle={USENIX Annual Technical Conference (ATC)},
  pages={97--112},
  year=2018,
  abstract={The virtual machine abstraction provides a wide variety of benefits which have undeniably enabled cloud computing. Virtual machines, however, are a doubleedged sword as hypervisors they run on top of must treat them as a black box, limiting the information which the hypervisor and virtual machine may exchange, a problem known as the semantic gap. In this paper, we present the design and implementation of a new mechanism, hyperupcalls, which enables a hypervisor to safely execute verified code provided by a guest virtual machine in order to transfer information. Hyperupcalls are written in C and have complete access to guest data structures such as page tables. We provide a complete framework which makes it easy to access familiar kernel functions from within a hyperupcall. Compared to state-of-the-art paravirtualization techniques and virtual machine introspection, Hyperupcalls are much more flexible and less intrusive. We demonstrate that hyperupcalls can not only be used to improve guest performance for certain operations by up to 2× but hyperupcalls can also serve as a powerful debugging and security tool.}
}

@inproceedings{aguilera2018remote,
  title={Remote regions: a simple abstraction for remote memory},
  author={Aguilera, Marcos K and Amit, Nadav and Calciu, Irina and Deguillard, Xavier and Gandhi, Jayneel and Novakovic, Stanko and Ramanathan, Arun and Subrahmanyam, Pratap and Suresh, Lalith and Tati, Kiran and others},
  booktitle={USENIX Annual Technical Conference (ATC)},
  year=2018,
  abstract={We propose an intuitive abstraction for a process to export its memory to remote hosts, and to access the memory exported by others. This abstraction provides a simpler interface to RDMA and other remote memory technologies compared to the existing verbs interface. The key idea is that a process can export parts of its memory as files, called remote regions, that can be accessed through the usual file system operations (read, write, memory map, etc). We built this abstraction and evaluated it. We show that remote regions are easy to use and perform close to RDMA. We demonstrate it via micro-benchmarks and by modifying two in-memory single-host applications to use remote memory: R and Metis. These modifications amount to ≈100 lines of code; they allow R to exceed the physical memory of a host while running fat; and they allow Metis scale its performance across 8 hosts.}
}

